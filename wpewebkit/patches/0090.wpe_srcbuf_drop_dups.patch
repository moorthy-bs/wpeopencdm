diff --git a/Source/WebCore/Modules/mediasource/MediaSource.cpp b/Source/WebCore/Modules/mediasource/MediaSource.cpp
index 16407326a..4f2e3fab0 100644
--- a/Source/WebCore/Modules/mediasource/MediaSource.cpp
+++ b/Source/WebCore/Modules/mediasource/MediaSource.cpp
@@ -50,8 +50,6 @@
 #include "TimeRanges.h"
 #include "VideoTrackList.h"
 
-namespace WebCore {
-
 #ifdef LOG
 #undef LOG
 #endif
@@ -60,10 +58,23 @@ namespace WebCore {
 #undef LOG_DISABLED
 #endif
 
-// #define LOG( x, s... )  ({ char _buf[4096] = { 0 }; int _l = 0; _l += snprintf(_buf+_l,sizeof(_buf)-_l," %4d | ["#x"] %p ",__LINE__,(void*)pthread_self()); _l += snprintf(_buf+_l,sizeof(_buf)-_l,"| " s); fprintf(stderr,"%s\n",_buf); })
-// #define LOG_DISABLED 0
+//#define LOG_DISABLED 1
+#define LOG_DISABLED 0
+
+#if !LOG_DISABLED
+
+#include <gst/gst.h>
+
+GST_DEBUG_CATEGORY_EXTERN(webkitmediasrc);
+#define GST_CAT_DEFAULT webkitmediasrc
+
+#define LOG(x, ...)  GST_DEBUG(__VA_ARGS__);
+
+#else /* !LOG_DISABLED */
 #define LOG( x, s... )  ({ })
-#define LOG_DISABLED 1
+#endif /* !LOG_DISABLED */
+
+namespace WebCore {
 
 #if !LOG_DISABLED
 static const char* toString(MediaSource::ReadyState readyState)
diff --git a/Source/WebCore/Modules/mediasource/SourceBuffer.cpp b/Source/WebCore/Modules/mediasource/SourceBuffer.cpp
index 727834733..f70bd5ee0 100644
--- a/Source/WebCore/Modules/mediasource/SourceBuffer.cpp
+++ b/Source/WebCore/Modules/mediasource/SourceBuffer.cpp
@@ -60,11 +60,6 @@
 #include <wtf/CurrentTime.h>
 #include <wtf/NeverDestroyed.h>
 
-namespace WebCore {
-
-static const double ExponentialMovingAverageCoefficient = 0.1;
-static const bool BufferedLegacyCalcuation = getenv("BUFFERED_LEGACY_CALCULATION") ? true : false;
-
 #ifdef LOG
 #undef LOG
 #endif
@@ -73,10 +68,36 @@ static const bool BufferedLegacyCalcuation = getenv("BUFFERED_LEGACY_CALCULATION
 #undef LOG_DISABLED
 #endif
 
-// #define LOG( x, s... )  ({ char _buf[4096] = { 0 }; int _l = 0; _l += snprintf(_buf+_l,sizeof(_buf)-_l," %4d | ["#x"] %p ",__LINE__,(void*)pthread_self()); _l += snprintf(_buf+_l,sizeof(_buf)-_l,"| " s); fprintf(stderr,"%s\n",_buf); })
-// #define LOG_DISABLED 0
+//#define LOG_DISABLED 1
+#define LOG_DISABLED 0
+
+#if !LOG_DISABLED
+
+#include <gst/gst.h>
+
+GST_DEBUG_CATEGORY(webkitmediasrc);
+#define GST_CAT_DEFAULT webkitmediasrc
+
+struct InitGstDebug
+{
+    InitGstDebug()
+    {
+        GST_DEBUG_CATEGORY_INIT(webkitmediasrc, "webkitmediasrc", 0, "mediasource module");
+    }
+};
+
+static InitGstDebug gInitGstDebug;
+
+#define LOG(x, ...)  GST_DEBUG(__VA_ARGS__);
+
+#else /* !LOG_DISABLED */
 #define LOG( x, s... )  ({ })
-#define LOG_DISABLED 1
+#endif /* !LOG_DISABLED */
+
+namespace WebCore {
+
+static const double ExponentialMovingAverageCoefficient = 0.1;
+static const bool BufferedLegacyCalcuation = getenv("BUFFERED_LEGACY_CALCULATION") ? true : false;
 
 struct SourceBuffer::TrackBuffer {
     MediaTime lastDecodeTimestamp;
@@ -91,15 +112,19 @@ struct SourceBuffer::TrackBuffer {
     DecodeOrderSampleMap::MapType decodeQueue;
     RefPtr<MediaDescription> description;
     PlatformTimeRanges buffered;
+    AtomicString trackID;
 
-    TrackBuffer()
+    TrackBuffer(AtomicString trackID)
         : lastDecodeTimestamp(MediaTime::invalidTime())
         , lastFrameDuration(MediaTime::invalidTime())
         , highestPresentationTimestamp(MediaTime::invalidTime())
         , lastEnqueuedPresentationTime(MediaTime::invalidTime())
         , lastEnqueuedDecodeEndTime(MediaTime::invalidTime())
-    {
-    }
+        , trackID{trackID}
+    {}
+
+    TrackBuffer(): TrackBuffer(AtomicString{})
+    {}
 };
 
 Ref<SourceBuffer> SourceBuffer::create(Ref<SourceBufferPrivate>&& sourceBufferPrivate, MediaSource* source)
@@ -272,10 +297,18 @@ void SourceBuffer::resetParserState()
     // 4. Unset the highest presentation timestamp on all track buffers.
     // 5. Set the need random access point flag on all track buffers to true.
     for (auto& trackBufferPair : m_trackBufferMap.values()) {
+
+        LOG(
+            MediaSource,
+            "%s HPTS %fs",
+            trackBufferPair.trackID.string().utf8().data(),
+            trackBufferPair.highestPresentationTimestamp.toDouble());
+
         trackBufferPair.lastDecodeTimestamp = MediaTime::invalidTime();
         trackBufferPair.lastFrameDuration = MediaTime::invalidTime();
         trackBufferPair.highestPresentationTimestamp = MediaTime::invalidTime();
         trackBufferPair.needRandomAccessFlag = true;
+        trackBufferPair.trackID = AtomicString{};
     }
     // 6. Remove all bytes from the input buffer.
     // Note: this is handled by abortIfUpdating()
@@ -398,12 +431,17 @@ void SourceBuffer::abortIfUpdating()
 
 MediaTime SourceBuffer::highestPresentationTimestamp() const
 {
-    MediaTime highestTime;
+    MediaTime highestTime = MediaTime::invalidTime();
+
     for (auto& trackBuffer : m_trackBufferMap.values()) {
+
         auto lastSampleIter = trackBuffer.samples.presentationOrder().rbegin();
         if (lastSampleIter == trackBuffer.samples.presentationOrder().rend())
             continue;
-        highestTime = std::max(highestTime, lastSampleIter->first);
+        highestTime =
+            highestTime.isValid()
+            ? std::max(highestTime, lastSampleIter->first)
+            : lastSampleIter->first;
     }
     return highestTime;
 }
@@ -437,6 +475,14 @@ void SourceBuffer::seekToTime(const MediaTime& time)
         TrackBuffer& trackBuffer = trackBufferPair.value;
         const AtomicString& trackID = trackBufferPair.key;
 
+    LOG(
+            MediaSource,
+            "%p %s %fs to %fs",
+            this,
+            trackID.string().utf8().data(),
+            MediaTime{g_get_monotonic_time(), GST_USECOND}.toDouble(),
+            time.toDouble());
+
         trackBuffer.needsReenqueueing = true;
         reenqueueMediaForTime(trackBuffer, trackID, time);
     }
@@ -642,10 +688,12 @@ void SourceBuffer::sourceBufferPrivateAppendComplete(AppendResult result)
     if (isRemoved())
         return;
 
+    AtomicString trackID;
+
     MediaTime currentMediaTime = m_source->currentTime();
     for (auto& trackBufferPair : m_trackBufferMap) {
         TrackBuffer& trackBuffer = trackBufferPair.value;
-        const AtomicString& trackID = trackBufferPair.key;
+        trackID = trackBufferPair.key;
 
         if (trackBuffer.needsReenqueueing) {
             LOG(MediaSource, "SourceBuffer::sourceBufferPrivateAppendComplete(%p) - reenqueuing at time (%s)", this, toString(currentMediaTime).utf8().data());
@@ -658,7 +706,12 @@ void SourceBuffer::sourceBufferPrivateAppendComplete(AppendResult result)
     if (extraMemoryCost() > this->maximumBufferSize())
         m_bufferFull = true;
 
-    LOG(Media, "SourceBuffer::sourceBufferPrivateAppendComplete(%p) - buffered = %s", this, toString(m_buffered->ranges()).utf8().data());
+    LOG(
+        Media,
+        "(%p) %s buffered %s",
+        this,
+        trackID.string().utf8().data(),
+        toString(m_buffered->ranges()).utf8().data());
 }
 
 void SourceBuffer::sourceBufferPrivateDidReceiveRenderingError(int error)
@@ -678,8 +731,16 @@ static bool decodeTimeComparator(const PresentationOrderSampleMap::MapType::valu
     return a.second->decodeTime() < b.second->decodeTime();
 }
 
-static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMap::MapType& samples, SourceBuffer::TrackBuffer& trackBuffer, const SourceBuffer* buffer, const char* logPrefix)
+static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMap::MapType& samples, SourceBuffer::TrackBuffer& trackBuffer, AtomicString trackID, const SourceBuffer* buffer, const char* logPrefix, bool keepDecodeQueue)
 {
+    LOG(
+        MediaSource,
+        "%s %p %s, samples num %zu",
+        logPrefix,
+        buffer,
+        trackID.string().utf8().data(),
+        samples.size());
+
 #if !LOG_DISABLED
     MediaTime earliestSample = MediaTime::positiveInfiniteTime();
     MediaTime latestSample = MediaTime::zeroTime();
@@ -700,13 +761,20 @@ static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMa
 #endif
 
         RefPtr<MediaSample>& sample = sampleIt.second;
-//         LOG(MediaSource, "SourceBuffer::%s(%p) - removing sample(%s)", logPrefix, buffer, toString(*sampleIt.second).utf8().data());
+
+        LOG(
+            MediaSource, "%p %s PTS %fs DTS %fs DUR %fs",
+            buffer,
+            sampleIt.second->trackID().string().utf8().data(),
+            sampleIt.second->presentationTime().toDouble(),
+            sampleIt.second->decodeTime().toDouble(),
+            sampleIt.second->duration().toDouble());
 
         // Remove the erased samples from the TrackBuffer sample map.
         trackBuffer.samples.removeSample(sample.get());
 
         // Also remove the erased samples from the TrackBuffer decodeQueue.
-        trackBuffer.decodeQueue.erase(decodeKey);
+        if(!keepDecodeQueue) trackBuffer.decodeQueue.erase(decodeKey);
 
         auto startTime = sample->presentationTime();
 #if USE(GSTREAMER)
@@ -755,13 +823,19 @@ static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMa
 
 #if !LOG_DISABLED
     if (bytesRemoved)
-        LOG(MediaSource, "SourceBuffer::%s(%p) removed %zu bytes, start(%lf), end(%lf)", logPrefix, buffer, bytesRemoved, earliestSample.toDouble(), latestSample.toDouble());
+        LOG(
+            MediaSource,
+            "%s (%p) %s removed %zu bytes, start %fs, end %fs",
+            logPrefix, buffer,
+            trackID.string().utf8().data(),
+            bytesRemoved,
+            earliestSample.toDouble(), latestSample.toDouble());
 #endif
 
     return erasedRanges;
 }
 
-void SourceBuffer::removeCodedFrames(const MediaTime& start, const MediaTime& end)
+void SourceBuffer::removeCodedFrames(const MediaTime& start, const MediaTime& end, bool keepDecodeQueue)
 {
     // 3.5.9 Coded Frame Removal Algorithm
     // https://dvcs.w3.org/hg/html-media/raw-file/tip/media-source/media-source.html#sourcebuffer-coded-frame-removal
@@ -851,11 +925,23 @@ void SourceBuffer::removeCodedFrames(const MediaTime& start, const MediaTime& en
             ? toString(removeDecodeEnd->second->presentationTime()).utf8().data()
             : "undefined");
 
-        PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(erasedSamples, trackBuffer, this, "removeCodedFrames");
+        PlatformTimeRanges erasedRanges =
+            removeSamplesFromTrackBuffer(
+                erasedSamples,
+                trackBuffer,
+                trackBuffer.trackID,
+                this,
+                "removeCodedFrames",
+                keepDecodeQueue);
 
         // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
         // not yet displayed samples.
-        if (trackBuffer.lastEnqueuedPresentationTime.isValid() && currentMediaTime < trackBuffer.lastEnqueuedPresentationTime) {
+        if(
+            trackBuffer.lastEnqueuedPresentationTime.isValid()
+            && currentMediaTime < trackBuffer.lastEnqueuedPresentationTime
+            && !hasAudio()
+            && !keepDecodeQueue)
+        {
             PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.lastEnqueuedPresentationTime);
             possiblyEnqueuedRanges.intersectWith(erasedRanges);
             if (possiblyEnqueuedRanges.length())
@@ -914,7 +1000,7 @@ void SourceBuffer::removeTimerFired()
     // http://w3c.github.io/media-source/#sourcebuffer-range-removal
 
     // 6. Run the coded frame removal algorithm with start and end as the start and end of the removal range.
-    removeCodedFrames(m_pendingRemoveStart, m_pendingRemoveEnd);
+    removeCodedFrames(m_pendingRemoveStart, m_pendingRemoveEnd, false);
 
     // 7. Set the updating attribute to false.
     m_updating = false;
@@ -965,7 +1051,7 @@ void SourceBuffer::evictRangeIfPossible(const MediaTime &begin, const MediaTime
         }
     }
 
-    removeCodedFrames(begin, end);
+    removeCodedFrames(begin, end, true);
 }
 
 void SourceBuffer::evictCodedFrames(size_t newDataSize)
@@ -983,13 +1069,17 @@ void SourceBuffer::evictCodedFrames(size_t newDataSize)
     // 2. If the buffer full flag equals false, then abort these steps.
     if (!m_bufferFull)
     {
+        const auto currBufSize = extraMemoryCost();
+
         LOG(
             MediaSource,
             "SourceBuffer::evictCodedFrames(%p)"
-            " not needed, currTime %s currBufSize %zu",
+            " not needed, currTime %fs buffered %s currBufSize %zuB (%zuMB)",
             this,
-            toString(currentTime).utf8().data(),
-            extraMemoryCost());
+            currentTime.toDouble(),
+            toString(m_buffered->ranges()).utf8().data(),
+            currBufSize,
+            currBufSize / (1024 * 1024));
         return;
     }
 
@@ -1300,7 +1390,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveInitializationSegment(const Init
 
             // 5.2.8 Create a new track buffer to store coded frames for this track.
             ASSERT(!m_trackBufferMap.contains(newAudioTrack->id()));
-            auto& trackBuffer = m_trackBufferMap.add(newAudioTrack->id(), TrackBuffer()).iterator->value;
+            auto& trackBuffer = m_trackBufferMap.add(newAudioTrack->id(), TrackBuffer(newAudioTrack->id())).iterator->value;
 
             // 5.2.9 Add the track description for this track to the track buffer.
             trackBuffer.description = audioTrackInfo.description;
@@ -1339,7 +1429,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveInitializationSegment(const Init
 
             // 5.3.8 Create a new track buffer to store coded frames for this track.
             ASSERT(!m_trackBufferMap.contains(newVideoTrack->id()));
-            auto& trackBuffer = m_trackBufferMap.add(newVideoTrack->id(), TrackBuffer()).iterator->value;
+            auto& trackBuffer = m_trackBufferMap.add(newVideoTrack->id(), TrackBuffer(newVideoTrack->id())).iterator->value;
 
             // 5.3.9 Add the track description for this track to the track buffer.
             trackBuffer.description = videoTrackInfo.description;
@@ -1375,7 +1465,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveInitializationSegment(const Init
 
             // 5.4.7 Create a new track buffer to store coded frames for this track.
             ASSERT(!m_trackBufferMap.contains(textTrackPrivate.id()));
-            auto& trackBuffer = m_trackBufferMap.add(textTrackPrivate.id(), TrackBuffer()).iterator->value;
+            auto& trackBuffer = m_trackBufferMap.add(textTrackPrivate.id(), TrackBuffer(textTrackPrivate.id())).iterator->value;
 
             // 5.4.8 Add the track description for this track to the track buffer.
             trackBuffer.description = textTrackInfo.description;
@@ -1526,6 +1616,31 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         return;
     }
 
+    // 1.5 Let track buffer equal the track buffer that the coded frame will be added to.
+    AtomicString trackID = sample.trackID();
+    auto it = m_trackBufferMap.find(trackID);
+
+    if (it == m_trackBufferMap.end()) {
+        // The client managed to append a sample with a trackID not present in the initialization
+        // segment. This would be a good place to post an message to the developer console.
+        didDropSample();
+        return;
+    }
+
+    TrackBuffer& trackBuffer = it->value;
+
+    LOG(
+        MediaSource,
+        "NEWSAMPLE %s %fs HPTS %fs LEPTS %fs PTS %fs DTS %fs DUR %fs %zub",
+        sample.trackID().string().utf8().data(),
+        MediaTime{g_get_monotonic_time(), GST_USECOND}.toDouble(),
+        trackBuffer.highestPresentationTimestamp.toDouble(),
+        trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+        sample.presentationTime().toDouble(),
+        sample.decodeTime().toDouble(),
+        sample.duration().toDouble(),
+        sample.sizeInBytes());
+
     // 3.5.8 Coded Frame Processing
     // http://www.w3.org/TR/media-source/#sourcebuffer-coded-frame-processing
 
@@ -1584,24 +1699,21 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
             decodeTimestamp += m_timestampOffset;
         }
 
-        // 1.5 Let track buffer equal the track buffer that the coded frame will be added to.
-        AtomicString trackID = sample.trackID();
-        auto it = m_trackBufferMap.find(trackID);
-        if (it == m_trackBufferMap.end()) {
-            // The client managed to append a sample with a trackID not present in the initialization
-            // segment. This would be a good place to post an message to the developer console.
-            didDropSample();
-            return;
-        }
-        TrackBuffer& trackBuffer = it->value;
-
-//         fprintf(stderr,"NEW SAMPLE[%d]: %lf, %lf ! %lf\n",m_mode,presentationTimestamp.toDouble(),frameDuration.toDouble(),trackBuffer.highestPresentationTimestamp.toDouble());
+        LOG(MediaSource,
+            "trackBuffer %s  HPTS %fs LFD %fs LEPTS %fs LEDET %fs LDT %fs",
+            trackID.string().utf8().data(),
+            trackBuffer.highestPresentationTimestamp.toDouble(),
+            trackBuffer.lastFrameDuration.toDouble(),
+            trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+            trackBuffer.lastEnqueuedDecodeEndTime.toDouble(),
+            trackBuffer.lastDecodeTimestamp.toDouble());
 
         // 1.6 ↳ If last decode timestamp for track buffer is set and decode timestamp is less than last
         // decode timestamp:
         // OR
         // ↳ If last decode timestamp for track buffer is set and the difference between decode timestamp and
         // last decode timestamp is greater than 2 times last frame duration:
+#if 0
         if (trackBuffer.lastDecodeTimestamp.isValid() && (decodeTimestamp < trackBuffer.lastDecodeTimestamp
             || abs(decodeTimestamp - trackBuffer.lastDecodeTimestamp) > (trackBuffer.lastFrameDuration * 2))) {
 
@@ -1622,7 +1734,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
                 // 1.6.3 Unset the last frame duration on all track buffers.
                 trackBuffer.lastFrameDuration = MediaTime::invalidTime();
                 // 1.6.4 Unset the highest presentation timestamp on all track buffers.
-//                 trackBuffer.highestPresentationTimestamp = MediaTime::invalidTime();
+                trackBuffer.highestPresentationTimestamp = MediaTime::invalidTime();
                 // 1.6.5 Set the need random access point flag on all track buffers to true.
                 trackBuffer.needRandomAccessFlag = true;
             }
@@ -1630,6 +1742,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
             // 1.6.6 Jump to the Loop Top step above to restart processing of the current coded frame.
             continue;
         }
+#endif
 
         if (m_mode == AppendMode::Sequence) {
             // Use the generated timestamps instead of the sample's timestamps.
@@ -1640,7 +1753,7 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         }
 
         // 1.7 Let frame end timestamp equal the sum of presentation timestamp and frame duration.
-        MediaTime frameEndTimestamp = presentationTimestamp + frameDuration;
+        const auto frameEndTimestamp = presentationTimestamp + frameDuration;
 
         // 1.8 If presentation timestamp is less than appendWindowStart, then set the need random access
         // point flag to true, drop the coded frame, and jump to the top of the loop to start processing
@@ -1650,6 +1763,14 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         // the next coded frame.
         if (presentationTimestamp < m_appendWindowStart || frameEndTimestamp > m_appendWindowEnd) {
             trackBuffer.needRandomAccessFlag = true;
+
+            LOG(
+                MediaSource,
+                "DROP %s PTS %fs DTS %fs DUR %fs",
+                trackID.string().utf8().data(),
+                sample.presentationTime().toDouble(),
+                sample.decodeTime().toDouble(),
+                sample.duration().toDouble());
             didDropSample();
             return;
         }
@@ -1712,8 +1833,15 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
 
                     // 1.14.2.3 If the presentation timestamp is less than the remove window timestamp,
                     // then remove overlapped frame and any coded frames that depend on it from track buffer.
-                    if (presentationTimestamp < removeWindowTimestamp) {
-//                         fprintf(stderr," %4d | REMOVE[%d]: %lf %lf\n",__LINE__,m_mode,presentationTimestamp.toDouble(),removeWindowTimestamp.toDouble());
+                    if (presentationTimestamp < removeWindowTimestamp)
+                    {
+                        LOG(
+                            MediaSource,
+                            "erase append %s PTS %fs RWT %fs sample PTS %fs",
+                            trackID.string().utf8().data(),
+                            presentationTimestamp.toDouble(),
+                            removeWindowTimestamp.toDouble(),
+                            iter->second->presentationTime().toDouble());
                         erasedSamples.addSample(*iter->second);
                     }
                 }
@@ -1730,8 +1858,17 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
             // Remove all coded frames from track buffer that have a presentation timestamp greater than or
             // equal to presentation timestamp and less than frame end timestamp.
             auto iter_pair = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimes(presentationTimestamp, frameEndTimestamp);
-            if (iter_pair.first != trackBuffer.samples.presentationOrder().end()) {
-//                 fprintf(stderr," %4d | REMOVE[%d]: %lf %lf\n",__LINE__,m_mode,presentationTimestamp.toDouble(),frameEndTimestamp.toDouble());
+            if (iter_pair.first != trackBuffer.samples.presentationOrder().end())
+            {
+                 LOG(
+                    MediaSource,
+                    "erase append %s PTS %fs FEND %fs range PTS from %fs to %fs",
+                    trackID.string().utf8().data(),
+                    presentationTimestamp.toDouble(),
+                    frameEndTimestamp.toDouble(),
+                    iter_pair.first->second->presentationTime().toDouble(),
+                    iter_pair.second != trackBuffer.samples.presentationOrder().end() ? iter_pair.second->second->presentationTime().toDouble() : MediaTime::invalidTime().toDouble());
+
                 erasedSamples.addRange(iter_pair.first, iter_pair.second);
             }
         }
@@ -1756,8 +1893,18 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
                 else
                     range = trackBuffer.samples.presentationOrder().findSamplesWithinPresentationRange(trackBuffer.highestPresentationTimestamp, frameEndTimestamp);
 
-                if (range.first != trackBuffer.samples.presentationOrder().end()) {
-//                     fprintf(stderr," %4d | REMOVE[%d]: %lf %lf\n",__LINE__,m_mode,trackBuffer.highestPresentationTimestamp.toDouble(),frameEndTimestamp.toDouble());
+                if (range.first != trackBuffer.samples.presentationOrder().end())
+                {
+                    LOG(
+                        MediaSource,
+                        "ERASE_RANGE_C %s HPTS %fs PTS %fs FEND %fs range PTS from %fs to %fs",
+                        trackID.string().utf8().data(),
+                        trackBuffer.highestPresentationTimestamp.toDouble(),
+                        presentationTimestamp.toDouble(),
+                        frameEndTimestamp.toDouble(),
+                        range.first->second->presentationTime().toDouble(),
+                        range.second != trackBuffer.samples.presentationOrder().end() ? range.second->second->presentationTime().toDouble() : MediaTime::invalidTime().toDouble());
+
                     erasedSamples.addRange(range.first, range.second);
                 }
             } while(false);
@@ -1773,28 +1920,50 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         // for me this looks like YT's issue becuase IMO it shouldn't push any data since .buffered() says the data is
         // already there. It might be YT expects the decoder to handle that gracefully. All in all such samples are
         // replaces old one (it's safer to take newly fed one).
+
+        auto bypassAppend = false;
+
         if (!m_source->mediaElement()->seeking() &&
             !m_source->mediaElement()->paused() &&
             m_source->mediaElement()->isPlaying()) {
             auto foundInSamples = trackBuffer.samples.presentationOrder().findSampleWithPresentationTime(presentationTimestamp);
             if (foundInSamples != trackBuffer.samples.presentationOrder().end()) {
-                WTFLogAlways("Replacing sample for %s because detected repeated frame (PTS:%f, old size: %fx%f, new size: %fx%f,"
-                             " old dur: %f, new dur:%f, old DTS:%f, new DTS:%f)\n",
-                             sample.trackID().string().utf8().data(),
-                             presentationTimestamp.toDouble(),
-                            foundInSamples->second->presentationSize().width(),
-                            foundInSamples->second->presentationSize().height(),
-                            sample.presentationSize().width(),
-                            sample.presentationSize().height(),
-                            foundInSamples->second->duration().toDouble(),
-                            sample.duration().toDouble(),
-                            foundInSamples->second->decodeTime().toDouble(),
-                            sample.decodeTime().toDouble());
-
-                 erasedSamples.addSample(*(foundInSamples->second));
+
+                auto &bufferedSample = foundInSamples->second;
+
+                const auto equalPTS = sample.presentationTime() == bufferedSample->presentationTime();
+                const auto equalDTS = sample.decodeTime() == bufferedSample->decodeTime();
+                const auto equalDUR = sample.duration() == bufferedSample->duration();
+                const auto equalSIZE = sample.sizeInBytes() == bufferedSample->sizeInBytes();
+                const auto presentSIZE = sample.presentationSize() == bufferedSample->presentationSize();
+                const auto equalSYNC = sample.isSync() == bufferedSample->isSync();
+                const auto equalNONDISP = sample.isNonDisplaying() == bufferedSample->isNonDisplaying();
+
+                bypassAppend =
+                    equalPTS && equalDTS && equalDUR && equalSIZE && presentSIZE && equalSYNC && equalNONDISP;
+
+                LOG(
+                    MediaSource,
+                    "dup (%d%d%d%d%d%d%d)? %s PTS:%fs DTS %fs DUR: %fs SIZE %zub"
+                    "-> %s PTS:%fs DTS %fs DUR: %fs SIZE %zub",
+                    equalPTS, equalDTS, equalDUR, equalSIZE, presentSIZE, equalSYNC, equalNONDISP,
+                    sample.trackID().string().utf8().data(),
+                    sample.presentationTime().toDouble(),
+                    sample.decodeTime().toDouble(),
+                    sample.duration().toDouble(),
+                    sample.sizeInBytes(),
+                    bufferedSample->trackID().string().utf8().data(),
+                    bufferedSample->presentationTime().toDouble(),
+                    bufferedSample->decodeTime().toDouble(),
+                    bufferedSample->duration().toDouble(),
+                    bufferedSample->sizeInBytes());
+
+                if(!bypassAppend) erasedSamples.addSample(*(foundInSamples->second));
             }
         }
 
+        if(bypassAppend) break;
+
         // 1.16 Remove decoding dependencies of the coded frames removed in the previous step:
         DecodeOrderSampleMap::MapType dependentSamples;
         if (!erasedSamples.empty()) {
@@ -1808,12 +1977,23 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
             auto nextSyncIter = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(lastDecodeIter);
             dependentSamples.insert(firstDecodeIter, nextSyncIter);
 
-            PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(dependentSamples, trackBuffer, this, "sourceBufferPrivateDidReceiveSample");
+            PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(dependentSamples, trackBuffer, trackID, this, "sourceBufferPrivateDidReceiveSample", false);
+
+            LOG(
+                MediaSource,
+                "erasedSamples %zuB "
+                "erasedRanges %s",
+                erasedSamples.sizeInBytes(),
+                toString(erasedRanges).utf8().data());
 
             // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
             // not yet displayed samples.
             MediaTime currentMediaTime = m_source->currentTime();
-            if (currentMediaTime < trackBuffer.lastEnqueuedPresentationTime) {
+            if(
+                trackBuffer.lastEnqueuedPresentationTime.isValid()
+                && currentMediaTime < trackBuffer.lastEnqueuedPresentationTime
+                && !hasAudio())
+            {
                 PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.lastEnqueuedPresentationTime);
                 possiblyEnqueuedRanges.intersectWith(erasedRanges);
                 if (possiblyEnqueuedRanges.length())
@@ -1835,9 +2015,18 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         // Add the coded frame with the presentation timestamp, decode timestamp, and frame duration to the track buffer.
         trackBuffer.samples.addSample(sample);
 
-//         if (trackBuffer.lastEnqueuedDecodeEndTime.isInvalid() || decodeTimestamp >= trackBuffer.lastEnqueuedDecodeEndTime)
+        if (trackBuffer.lastEnqueuedPresentationTime.isInvalid() || presentationTimestamp >= trackBuffer.lastEnqueuedPresentationTime)
         {
             DecodeOrderSampleMap::KeyType decodeKey(decodeTimestamp, presentationTimestamp);
+
+            LOG(
+                MediaSource,
+                "INSERT %s PTS %fs DTS %fs DUR %fs",
+                sample.trackID().string().utf8().data(),
+                sample.presentationTime().toDouble(),
+                sample.decodeTime().toDouble(),
+                sample.duration().toDouble());
+
             trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, &sample));
         }
 
@@ -1850,8 +2039,29 @@ void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample& sample)
         // 1.20 If highest presentation timestamp for track buffer is unset or frame end timestamp is greater
         // than highest presentation timestamp, then set highest presentation timestamp for track buffer
         // to frame end timestamp.
+        if(trackBuffer.highestPresentationTimestamp.isInvalid())
+        {
+            trackBuffer.highestPresentationTimestamp = highestPresentationTimestamp();
+
+            LOG(
+                MediaSource,
+                "%s update HPTS %fs",
+                trackID.string().utf8().data(),
+                trackBuffer.highestPresentationTimestamp.toDouble());
+        }
+
         if (trackBuffer.highestPresentationTimestamp.isInvalid() || frameEndTimestamp > trackBuffer.highestPresentationTimestamp)
+        {
+            LOG(
+                MediaSource,
+                "%s override HPTS %fs with %fs, PTS %fs DUR %fs",
+                trackID.string().utf8().data(),
+                trackBuffer.highestPresentationTimestamp.toDouble(),
+                frameEndTimestamp.toDouble(),
+                presentationTimestamp.toDouble(),
+                frameDuration.toDouble());
             trackBuffer.highestPresentationTimestamp = frameEndTimestamp;
+        }
 
         // 1.21 If frame end timestamp is greater than group end timestamp, then set group end timestamp equal
         // to frame end timestamp.
@@ -2022,7 +2232,13 @@ void SourceBuffer::textTrackKindChanged(TextTrack& track)
 
 void SourceBuffer::sourceBufferPrivateDidBecomeReadyForMoreSamples(const AtomicString& trackID)
 {
-    LOG(MediaSource, "SourceBuffer::sourceBufferPrivateDidBecomeReadyForMoreSamples(%p)", this);
+    if (isRemoved()) return;
+
+    LOG(
+        MediaSource,
+        "(%p) %s",
+        this, trackID.string().utf8().data());
+
     auto it = m_trackBufferMap.find(trackID);
     if (it == m_trackBufferMap.end())
         return;
@@ -2037,13 +2253,41 @@ void SourceBuffer::provideMediaData(TrackBuffer& trackBuffer, const AtomicString
     if (m_source->isSeeking())
         return;
 
+    LOG(
+        MediaSource,
+        "(%p) "
+        "%s "
+        "HPTS %fs "
+        "LDT %fs "
+        "LEPTS %fs "
+        "LEDET %fs "
+        "buffered %s",
+        this,
+        trackID.string().utf8().data(),
+        trackBuffer.highestPresentationTimestamp.toDouble(),
+        trackBuffer.lastDecodeTimestamp.toDouble(),
+        trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+        trackBuffer.lastEnqueuedDecodeEndTime.toDouble(),
+        toString(m_buffered->ranges()).utf8().data());
+
+
 #if !LOG_DISABLED
     unsigned enqueuedSamples = 0;
 #endif
+    MediaTime lastEnqueuedPresentationTime = MediaTime::invalidTime();
+    MediaTime lastEnqueuedDecodeEndTime = MediaTime::invalidTime();
+    const auto currTime = m_source->currentTime();
 
     while (!trackBuffer.decodeQueue.empty()) {
         if (!m_private->isReadyForMoreSamples(trackID)) {
             m_private->notifyClientWhenReadyForMoreSamples(trackID);
+
+            LOG(
+                MediaSource,
+                "(%p) trackID %s not ready for more samples",
+                this,
+                trackID.string().utf8().data());
+
             break;
         }
 
@@ -2060,25 +2304,98 @@ void SourceBuffer::provideMediaData(TrackBuffer& trackBuffer, const AtomicString
         // new current time without triggering this early return.
         // FIXME(135867): Make this gap detection logic less arbitrary.
         MediaTime threshold(200, 1000);  // 200ms
+
         if (
             trackBuffer.lastEnqueuedPresentationTime.isValid()
             && sample->presentationTime() - trackBuffer.lastEnqueuedPresentationTime > threshold)
+        {
+            LOG(
+                MediaSource,
+                "(%p) track %s LEPTS %fs",
+                this,
+                trackID.string().utf8().data(),
+                trackBuffer.lastEnqueuedPresentationTime.toDouble());
             break;
+        }
 
         trackBuffer.decodeQueue.erase(trackBuffer.decodeQueue.begin());
         trackBuffer.lastEnqueuedPresentationTime = sample->presentationTime();
         trackBuffer.lastEnqueuedDecodeEndTime = sample->decodeTime() + sample->duration();
+        lastEnqueuedPresentationTime = sample->presentationTime();
+        lastEnqueuedDecodeEndTime =sample->decodeTime() + sample->duration();
+
+        LOG(
+            MediaSource,
+            "%p %fs %s PTS %fs DTS %fs DUR %fs %zub",
+            this,
+            MediaTime{g_get_monotonic_time(), GST_USECOND}.toDouble(),
+            trackID.string().utf8().data(),
+            sample->presentationTime().toDouble(),
+            sample->decodeTime().toDouble(),
+            sample->duration().toDouble(),
+            sample->sizeInBytes());
+
         m_private->enqueueSample(sample.releaseNonNull(), trackID);
 #if !LOG_DISABLED
         ++enqueuedSamples;
 #endif
     }
 
-    LOG(MediaSource, "SourceBuffer::provideMediaData(%p) - Enqueued %u samples", this, enqueuedSamples);
+    LOG(
+        MediaSource,
+        "%p "
+        "%s "
+        "%fs "
+        "currTime %fs "
+        "decodeQueue.size %zub "
+        "HPTS %fs "
+        "LDT %fs "
+        "LEPTS %fs "
+        "LEDET %fs "
+        "buffered %s"
+        "enqueued %u",
+        this,
+        trackID.string().utf8().data(),
+        MediaTime{g_get_monotonic_time(), GST_USECOND}.toDouble(),
+        currTime.toDouble(),
+        trackBuffer.decodeQueue.size(),
+        trackBuffer.highestPresentationTimestamp.toDouble(),
+        trackBuffer.lastDecodeTimestamp.toDouble(),
+        trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+        trackBuffer.lastEnqueuedDecodeEndTime.toDouble(),
+        toString(m_buffered->ranges()).utf8().data(),
+        enqueuedSamples);
 }
 
 void SourceBuffer::reenqueueMediaForTime(TrackBuffer& trackBuffer, const AtomicString& trackID, const MediaTime& time)
 {
+    LOG(
+        MediaSource,
+        "%p "
+        "time %fs "
+        "%s "
+        "decodeQueue.size %zub "
+        "HPTS %fs "
+        "LEPTS %fs "
+        "LEDET %fs  "
+        "buffered %s",
+        this, time.toDouble(),
+        trackID.string().utf8().data(),
+        trackBuffer.decodeQueue.size(),
+        trackBuffer.highestPresentationTimestamp.toDouble(),
+        trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+        trackBuffer.lastEnqueuedDecodeEndTime.toDouble(),
+        toString(m_buffered->ranges()).utf8().data());
+
+    const MediaTime currentMediaTime = m_source->currentTime();
+    const auto beginTimestamp = g_get_monotonic_time();
+
+    LOG(
+            MediaSource,
+            "%s %fs",
+            trackID.string().utf8().data(),
+            currentMediaTime.toDouble());
+
     m_private->flush(trackID);
     trackBuffer.decodeQueue.clear();
 
@@ -2106,24 +2423,70 @@ void SourceBuffer::reenqueueMediaForTime(TrackBuffer& trackBuffer, const AtomicS
     for (auto iter = reverseLastSyncSampleIter; iter != reverseCurrentSampleIter; --iter) {
         auto copy = iter->second->createNonDisplayingCopy();
         DecodeOrderSampleMap::KeyType decodeKey(copy->decodeTime(), copy->presentationTime());
+
+        LOG(
+            MediaSource,
+            "decodeQueue insert nondisplay %s PTS %fs DTS %fs DUR %fs %zub",
+            copy->trackID().string().utf8().data(),
+            copy->presentationTime().toDouble(),
+            copy->decodeTime().toDouble(),
+            copy->duration().toDouble(),
+            copy->sizeInBytes());
+
         trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, WTFMove(copy)));
     }
 
     if (!trackBuffer.decodeQueue.empty()) {
         auto& lastSample = trackBuffer.decodeQueue.rbegin()->second;
         trackBuffer.lastEnqueuedPresentationTime = lastSample->presentationTime();
-        trackBuffer.lastEnqueuedDecodeEndTime = lastSample->decodeTime();
+        trackBuffer.lastEnqueuedDecodeEndTime = lastSample->decodeTime() + lastSample->duration();
     } else {
         trackBuffer.lastEnqueuedPresentationTime = MediaTime::invalidTime();
         trackBuffer.lastEnqueuedDecodeEndTime = MediaTime::invalidTime();
     }
 
+    LOG(
+        MediaSource,
+        "%p "
+        "time %fs "
+        "%s "
+        "decodeQueue.size %zu "
+        "HPTS %fs "
+        "LEPTS %fs "
+        "LEDET %fs "
+        "buffered %s",
+        this,
+        time.toDouble(),
+        trackID.string().utf8().data(),
+        trackBuffer.decodeQueue.size(),
+        trackBuffer.highestPresentationTimestamp.toDouble(),
+        trackBuffer.lastEnqueuedPresentationTime.toDouble(),
+        trackBuffer.lastEnqueuedDecodeEndTime.toDouble(),
+        toString(m_buffered->ranges()).utf8().data());
+
     // Fill the decode queue with the remaining samples.
     for (auto iter = currentSampleDTSIterator; iter != trackBuffer.samples.decodeOrder().end(); ++iter)
+    {
+        LOG(
+            MediaSource,
+            "decodeQueue insert %s PTS %fs DTS %fs DUR %fs %zub",
+            iter->second->trackID().string().utf8().data(),
+            iter->second->presentationTime().toDouble(),
+            iter->second->decodeTime().toDouble(),
+            iter->second->duration().toDouble(),
+            iter->second->sizeInBytes());
+
         trackBuffer.decodeQueue.insert(*iter);
+    }
     provideMediaData(trackBuffer, trackID);
 
     trackBuffer.needsReenqueueing = false;
+
+   LOG(
+            MediaSource,
+            "%s diff %" PRId64 "us",
+            trackID.string().utf8().data(),
+            g_get_monotonic_time() - beginTimestamp);
 }
 
 
@@ -2174,6 +2537,10 @@ void SourceBuffer::updateBufferedFromTrackBuffers()
     for (auto& trackBuffer : m_trackBufferMap.values()) {
         // 4.1 Let track ranges equal the track buffer ranges for the current track buffer.
         PlatformTimeRanges trackRanges = trackBuffer.buffered;
+
+        if (!trackRanges.length())
+            continue;
+
         // 4.2 If readyState is "ended", then set the end time on the last range in track ranges to highest end time.
         if (m_source->isEnded())
             trackRanges.add(trackRanges.maximumBufferedTime(), highestEndTime);
diff --git a/Source/WebCore/Modules/mediasource/SourceBuffer.h b/Source/WebCore/Modules/mediasource/SourceBuffer.h
index e84fdc207..8da4cd1c1 100644
--- a/Source/WebCore/Modules/mediasource/SourceBuffer.h
+++ b/Source/WebCore/Modules/mediasource/SourceBuffer.h
@@ -170,7 +170,7 @@ private:
     void monitorBufferingRate();
 
     void removeTimerFired();
-    void removeCodedFrames(const MediaTime& start, const MediaTime& end);
+    void removeCodedFrames(const MediaTime& start, const MediaTime& end, bool);
 
     size_t extraMemoryCost() const;
     void reportExtraMemoryAllocated();
diff --git a/Source/WebCore/platform/graphics/gstreamer/MediaPlayerPrivateGStreamerBase.cpp b/Source/WebCore/platform/graphics/gstreamer/MediaPlayerPrivateGStreamerBase.cpp
index 318789fa1..ef5094c29 100644
--- a/Source/WebCore/platform/graphics/gstreamer/MediaPlayerPrivateGStreamerBase.cpp
+++ b/Source/WebCore/platform/graphics/gstreamer/MediaPlayerPrivateGStreamerBase.cpp
@@ -870,8 +870,8 @@ void MediaPlayerPrivateGStreamerBase::setVolume(float volume)
     if (!m_volumeElement)
         return;
 
-    GST_DEBUG("Setting volume: %f", volume);
-    gst_stream_volume_set_volume(m_volumeElement.get(), GST_STREAM_VOLUME_FORMAT_LINEAR, static_cast<double>(volume));
+    GST_DEBUG("setting volume: %f", volume);
+    gst_stream_volume_set_volume(m_volumeElement.get(), GST_STREAM_VOLUME_FORMAT_LINEAR, gdouble(volume));
 }
 
 float MediaPlayerPrivateGStreamerBase::volume() const
@@ -889,17 +889,20 @@ void MediaPlayerPrivateGStreamerBase::notifyPlayerOfVolumeChange()
         return;
     double volume;
     volume = gst_stream_volume_get_volume(m_volumeElement.get(), GST_STREAM_VOLUME_FORMAT_LINEAR);
+
+    GST_DEBUG("volume %f clamped to: %f", volume, CLAMP(volume, 0.0, 1.0));
+
     // get_volume() can return values superior to 1.0 if the user
     // applies software user gain via third party application (GNOME
     // volume control for instance).
     volume = CLAMP(volume, 0.0, 1.0);
-    m_player->volumeChanged(static_cast<float>(volume));
+    m_player->volumeChanged(float(volume));
 }
 
 void MediaPlayerPrivateGStreamerBase::volumeChangedCallback(MediaPlayerPrivateGStreamerBase* player)
 {
     // This is called when m_volumeElement receives the notify::volume signal.
-    GST_DEBUG("Volume changed to: %f", player->volume());
+    GST_DEBUG("volume %f (%s)", player->volume(), player->muted() ? "muted" : "not muted");
 
     player->m_notifier.notify(MainThreadNotification::VolumeChanged, [player] { player->notifyPlayerOfVolumeChange(); });
 }
@@ -924,6 +927,8 @@ void MediaPlayerPrivateGStreamerBase::setMuted(bool muted)
     if (!m_volumeElement)
         return;
 
+    GST_DEBUG("%d", muted);
+
     g_object_set(m_volumeElement.get(), "mute", muted, nullptr);
 }
 
@@ -932,8 +937,9 @@ bool MediaPlayerPrivateGStreamerBase::muted() const
     if (!m_volumeElement)
         return false;
 
-    bool muted;
+    gboolean muted;
     g_object_get(m_volumeElement.get(), "mute", &muted, nullptr);
+    GST_DEBUG("%d", muted);
     return muted;
 }
 
@@ -944,12 +950,14 @@ void MediaPlayerPrivateGStreamerBase::notifyPlayerOfMute()
 
     gboolean muted;
     g_object_get(m_volumeElement.get(), "mute", &muted, nullptr);
-    m_player->muteChanged(static_cast<bool>(muted));
+    GST_DEBUG("%d", muted);
+    m_player->muteChanged(bool(muted));
 }
 
 void MediaPlayerPrivateGStreamerBase::muteChangedCallback(MediaPlayerPrivateGStreamerBase* player)
 {
     // This is called when m_volumeElement receives the notify::mute signal.
+    GST_DEBUG("volume %f (%s)", player->volume(), player->muted() ? "muted" : "not muted");
     player->m_notifier.notify(MainThreadNotification::MuteChanged, [player] { player->notifyPlayerOfMute(); });
 }
 
diff --git a/Source/WebCore/platform/graphics/gstreamer/mse/PlaybackPipeline.cpp b/Source/WebCore/platform/graphics/gstreamer/mse/PlaybackPipeline.cpp
index 13085b9ee..f2c7adec9 100644
--- a/Source/WebCore/platform/graphics/gstreamer/mse/PlaybackPipeline.cpp
+++ b/Source/WebCore/platform/graphics/gstreamer/mse/PlaybackPipeline.cpp
@@ -57,6 +57,11 @@ static Stream* getStreamByTrackId(WebKitMediaSrc* source, AtomicString trackIdSt
             return stream;
         }
     }
+
+    GST_WARNING(
+        "%p %s no matching stream",
+        source,
+        trackIdString.string().utf8().data());
     return nullptr;
 }
 
@@ -66,6 +71,11 @@ static Stream* getStreamBySourceBufferPrivate(WebKitMediaSrc* source, WebCore::S
         if (stream->sourceBuffer == sourceBufferPrivate)
             return stream;
     }
+    GST_WARNING(
+        "%p no matching SourceBufferPrivateGStreamer %p %s",
+        source,
+        sourceBufferPrivate,
+        sourceBufferPrivate->trackId().string().utf8().data());
     return nullptr;
 }
 
@@ -184,7 +194,7 @@ void PlaybackPipeline::attachTrack(RefPtr<SourceBufferPrivateGStreamer> sourceBu
 
     const gchar* mediaType = gst_structure_get_name(structure);
 
-    GST_DEBUG_OBJECT(webKitMediaSrc, "Configured track %s: appsrc=%s, padId=%u, mediaType=%s", trackPrivate->id().string().utf8().data(), GST_ELEMENT_NAME(stream->appsrc), padId, mediaType);
+    GST_DEBUG_OBJECT(webKitMediaSrc, "%s appsrc %s, padId %u, mediaType %s", trackPrivate->id().string().utf8().data(), GST_ELEMENT_NAME(stream->appsrc), padId, mediaType);
 
     GUniquePtr<gchar> parserBinName(g_strdup_printf("streamparser%u", padId));
 
@@ -312,12 +322,20 @@ void PlaybackPipeline::attachTrack(RefPtr<SourceBufferPrivateGStreamer> sourceBu
 
     if (signal != -1)
         g_signal_emit(G_OBJECT(stream->parent), webKitMediaSrcSignals[signal], 0, nullptr);
+
+    if (caps) {
+        // Set caps to trigger early pipeline initialization
+        gst_app_src_set_caps(GST_APP_SRC(stream->appsrc), caps);
+
+        // Change the 'max_bytes' to signal internal condition and send caps down the stream
+        guint64 maxBytes = gst_app_src_get_max_bytes (GST_APP_SRC(stream->appsrc));
+        gst_app_src_set_max_bytes(GST_APP_SRC(stream->appsrc), maxBytes + 1);
+        gst_app_src_set_max_bytes(GST_APP_SRC(stream->appsrc), maxBytes);
+    }
 }
 
 void PlaybackPipeline::reattachTrack(RefPtr<SourceBufferPrivateGStreamer> sourceBufferPrivate, RefPtr<TrackPrivateBase> trackPrivate)
 {
-    GST_DEBUG("Re-attaching track");
-
     // FIXME: Maybe remove this method. Now the caps change is managed by gst_appsrc_push_sample() in enqueueSample()
     // and flushAndEnqueueNonDisplayingSamples().
 
@@ -335,6 +353,8 @@ void PlaybackPipeline::reattachTrack(RefPtr<SourceBufferPrivateGStreamer> source
     const gchar* mediaType = gst_structure_get_name(gst_caps_get_structure(appsrcCaps.get(), 0));
     int signal = -1;
 
+    GST_DEBUG_OBJECT(webKitMediaSrc, "medaType %s", mediaType);
+
     GST_OBJECT_LOCK(webKitMediaSrc);
     if (g_str_has_prefix(mediaType, "audio")) {
         ASSERT(stream->type == Audio);
@@ -415,7 +435,9 @@ void PlaybackPipeline::flush(AtomicString trackId)
 {
     ASSERT(WTF::isMainThread());
 
-    GST_DEBUG("flush: trackId=%s", trackId.string().utf8().data());
+    const auto beginTimestamp = g_get_monotonic_time();
+
+    GST_DEBUG_OBJECT(pipeline(), "%s", trackId.string().utf8().data());
 
     GST_OBJECT_LOCK(m_webKitMediaSrc.get());
     Stream* stream = getStreamByTrackId(m_webKitMediaSrc.get(), trackId);
@@ -429,17 +451,30 @@ void PlaybackPipeline::flush(AtomicString trackId)
     GstElement* appsrc = stream->appsrc;
     GST_OBJECT_UNLOCK(m_webKitMediaSrc.get());
 
-    if (!appsrc)
+    if(!appsrc) return;
+
+    /* bypass AUDIO flush */
+#if 0
+    if(trackId.startsWith("A"))
+    {
+        GST_DEBUG_OBJECT(
+            pipeline(),
+            "skipped %s %s (audio)",
+            GST_ELEMENT_NAME(appsrc),
+            trackId.string().utf8().data());
         return;
+    }
+#endif
 
     gint64 position = GST_CLOCK_TIME_NONE;
     GRefPtr<GstQuery> query = adoptGRef(gst_query_new_position(GST_FORMAT_TIME));
     if (gst_element_query(pipeline(), query.get()))
         gst_query_parse_position(query.get(), 0, &position);
 
-    GST_TRACE("Position: %" GST_TIME_FORMAT, GST_TIME_ARGS(position));
+    GST_DEBUG_OBJECT(pipeline(), "position %fs", MediaTime{position, GST_SECOND}.toDouble());
 
-    if (static_cast<guint64>(position) == GST_CLOCK_TIME_NONE) {
+    if (!GST_CLOCK_TIME_IS_VALID(position))
+    {
         GST_TRACE("Can't determine position, avoiding flush");
         return;
     }
@@ -474,8 +509,8 @@ void PlaybackPipeline::flush(AtomicString trackId)
     gst_segment_do_seek(segment.get(), rate, GST_FORMAT_TIME, GST_SEEK_FLAG_NONE,
         GST_SEEK_TYPE_SET, position, GST_SEEK_TYPE_SET, stop, nullptr);
 
-    GRefPtr<GstPad> sinkPad = gst_element_get_static_pad(appsrc, "src");
-    GRefPtr<GstPad> srcPad = sinkPad ? gst_pad_get_peer(sinkPad.get()) : nullptr;
+    GRefPtr<GstPad> sinkPad = adoptGRef(gst_element_get_static_pad(appsrc, "src"));
+    GRefPtr<GstPad> srcPad = sinkPad ? adoptGRef(gst_pad_get_peer(sinkPad.get())) : nullptr;
     if (srcPad)
         gst_pad_add_probe(srcPad.get(), GST_PAD_PROBE_TYPE_EVENT_DOWNSTREAM, segmentFixerProbe, nullptr, nullptr);
 
@@ -487,7 +522,11 @@ void PlaybackPipeline::flush(AtomicString trackId)
         return;
     }
 
-    GST_DEBUG("trackId=%s flushed", trackId.string().utf8().data());
+    const auto endTimestamp = g_get_monotonic_time();
+    const auto diff = MediaTime{endTimestamp - beginTimestamp, GST_USECOND};
+
+
+    GST_DEBUG_OBJECT(pipeline(), "%s took %fus", trackId.string().utf8().data(), diff.toDouble());
 }
 
 void PlaybackPipeline::enqueueSample(Ref<MediaSample>&& mediaSample)
